#! /usr/bin/python
"""
Graph sar(1) reports.

sar(1) provides system activity reports that are useful in the analysis of
system performance issues. This script produces a PDF file with graphs of the
data contained in one or more sar reports.
"""
# sarstats.py - sar(1) report graphing utility
# Copyright (C) 2012  Ray Dassen
#               2013  Ray Dassen, Michele Baldessari
#               2014  Michele Baldessari
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
# MA 02110-1301, USA.

from __future__ import print_function
from itertools import repeat
import sys
import argparse
import csv
import hashlib
import os
import re
import shutil
import tempfile
from hashlib import sha1
import multiprocessing

import dateutil
from reportlab.lib.styles import ParagraphStyle as PS
from reportlab.platypus import PageBreak, Image, Spacer
from reportlab.platypus.paragraph import Paragraph
from reportlab.platypus.doctemplate import PageTemplate, BaseDocTemplate
from reportlab.platypus.tableofcontents import TableOfContents
from reportlab.platypus.frames import Frame
from reportlab.lib.units import inch
from reportlab.lib.pagesizes import A4, landscape
import SAR
import sar_metadata


# If we should try and create the graphs in parallel
# brings a nice speedup on multi-core/smp machines
THREADED = True
# None means nr of available CPUs
NR_CPUS = None

# No more than the following nr of graphs in a single page
# per default
MAXGRAPHS_IN_PAGE = 64

DEFAULT_IMG_EXT = ".png"

# Inch graph size
GRAPH_WIDTH = 10.5
GRAPH_HEIGHT = 6.5


def natural_sort_key(string):
    """Natural sorting"""
    _nsre = re.compile('([0-9]+)')
    return [int(text) if text.isdigit() else text.lower() for text in re.split(_nsre, string)]


def ascii_date(d):
    return "%s" % (d.strftime("%Y-%m-%d %H:%M"))


def split_chunks(list_to_split, chunksize):
    """Split the list l in chunks of at most n in size"""
    return [list_to_split[i:i+chunksize] for i in range(0, len(list_to_split), chunksize)]


class MyDocTemplate(BaseDocTemplate):
    """Custom Doc Template in order to have bookmarks
    for certain type of text"""
    def __init__(self, filename, **kw):
        self.allowSplitting = 0
        apply(BaseDocTemplate.__init__, (self, filename), kw)
        template = PageTemplate('normal', [Frame(0.1*inch, 0.1*inch,
                                11*inch, 8*inch, id='F1')])
        self.addPageTemplates(template)

        self.centered = PS(
            name='centered',
            fontSize=30,
            leading=16,
            alignment=1,
            spaceAfter=20)

        self.centered_index = PS(
            name='centered_index',
            fontSize=24,
            leading=16,
            alignment=1,
            spaceAfter=20)

        self.small_centered = PS(
            name='small_centered',
            fontSize=14,
            leading=16,
            alignment=1,
            spaceAfter=20)

        self.h1 = PS(
            name='Heading1',
            fontSize=16,
            leading=16)

        self.h2 = PS(
            name='Heading2',
            fontSize=14,
            leading=14)

        self.h2_center = PS(
            name='Heading2Center',
            alignment=1,
            fontSize=14,
            leading=14)

        self.h2_invisible = PS(
            name='Heading2Invisible',
            alignment=1,
            textColor='#FFFFFF',
            fontSize=14,
            leading=14)

        self.mono = PS(
            name='Mono',
            fontName='Courier',
            fontSize=16,
            leading=16)

        self.normal = PS(
            name='Normal',
            fontSize=16,
            leading=16)

        self.toc = TableOfContents()
        self.toc.levelStyles = [
            PS(fontName='Times-Bold', fontSize=14, name='TOCHeading1',
                leftIndent=20, firstLineIndent=-20, spaceBefore=2, leading=16),
            PS(fontSize=10, name='TOCHeading2', leftIndent=40,
                firstLineIndent=-20, spaceBefore=0, leading=8),
        ]

    def afterFlowable(self, flowable):
        """Registers TOC entries."""
        if flowable.__class__.__name__ == 'Paragraph':
            text = flowable.getPlainText()
            style = flowable.style.name
            if style in ['Heading1', 'centered_index']:
                level = 0
            elif style in ['Heading2', 'Heading2Center', 'Heading2Invisible']:
                level = 1
            else:
                return
            entry = [level, text, self.page]
            #if we have a bookmark name append that to our notify data
            bookmark_name = getattr(flowable, '_bookmarkName', None)
            if bookmark_name is not None:
                entry.append(bookmark_name)
            self.notify('TOCEntry', tuple(entry))
            self.canv.addOutlineEntry(text, bookmark_name, level, True)


def graph_wrapper((sar_stats_obj, sar_obj, dataname)):
    """This is a wrapper due to pool.map() single argument limit"""
    fname = sar_stats_obj._graph_filename(dataname[1][0])
    sar_obj.plottimeseries(dataname, fname, sar_stats_obj.extra_labels,
            sar_stats_obj.showreboots, sar_stats_obj.grid)
    sys.stdout.write(".")
    sys.stdout.flush()


class SarStats(object):
    """Creates a pdf file given a parsed SAR object"""
    def __init__(self, sar_obj, maxgraphs = MAXGRAPHS_IN_PAGE):
        """Initialize class"""
        # Temporary dir where images are stored (one per graph)
        # NB: This is done to keep the memory usage constant
        # in spite of being a bit slower (before this change
        # we could use > 12GB RAM for a simple sar file)
        self._tempdir = tempfile.mkdtemp(prefix='sargrapher')
        self.story = []
        self.maxgraphs = maxgraphs
        self.sar_obj = sar_obj

    def close(self):
        """Removes temporary directory and files"""
        if os.path.isdir(self._tempdir):
            shutil.rmtree(self._tempdir)

    def _graph_filename(self, graph):
        """Creates a unique constant file name given a graph or graph list"""
        if isinstance(graph, list):
            temp = "_".join(graph)
        else:
            temp = graph
        temp = temp.replace('%', '_')
        temp = temp.replace('/', '_')
        digest = hashlib.sha1()
        digest.update(temp)
        fname = os.path.join(self._tempdir, digest.hexdigest() + DEFAULT_IMG_EXT)
        return fname

    def graphs_order(self, cat, skiplist=[]):
        """ Order in which to present all graphs.
        Data is grouped loosely by type. """
        l = []
        sar = self.sar_obj
        # First we add all the simple graphs sorted by chosen category list
        for i in cat:
            for j in sorted(sar.available_data_types(), key=SAR.natural_sort_key):
                # We cannot graph a column with device names
                if j.endswith('DEVICE'):
                    continue
                if sar_metadata.get_category(j) == i:
                    l.append([j])

        # Here we add the combined graphs always per category
        c = {}
        for i in sar_metadata.INDEX_COLUMN:
            s = sar.datanames_per_arg(i, False)
            try:
                key = sar_metadata.get_category(s[0][0])
            except:
                continue
            if not key in c:
                c[key] = s
            else:
                c[key] += s

        # We merge the two in a single list: for each category
        # simple graphs and then combined graphs
        l = []
        for i in cat:
            for j in sorted(sar.available_data_types(), key=SAR.natural_sort_key):
                if j in sar_metadata.BASE_GRAPHS and \
                    sar_metadata.BASE_GRAPHS[j]['cat'] == i and \
                    j not in skiplist:
                    entry = sar_metadata.get_title_unit_labels([j], sar_obj=self.sar_obj)
                    l.append([entry, [j]])
            if i in c:
                for j in range(len(c[i])):
                    # Only add the graph if none of it's components is in the skip_list
                    b = sorted([x for x in c[i][j] if len(set(skiplist).intersection(x.split('#'))) == 0],
                            key=SAR.natural_sort_key)
                    # If the graph has more than X columns we split it
                    if len(b) > self.maxgraphs:
                        chunks = split_chunks(b, self.maxgraphs)
                        counter = 1
                        for chunk in chunks:
                            entry = sar_metadata.get_title_unit_label(chunk, sar_obj=self.sar_obj)
                            s = "{0} {1}/{2}".format(entry[0], counter, len(chunks))
                            newentry = (s, entry[1])
                            l.append(([newentry, chunk]))
                            counter += 1
                    else:
                        entry = sar_metadata.get_title_unit_labels(b, sar_obj=self.sar_obj)
                        l.append([entry, b])

        return l

    def do_heading(self, text, sty):
        # create bookmarkname
        bn = sha1(text + sty.name).hexdigest()
        # modify paragraph text to include an anchor point with name bn
        h = Paragraph(text + '<a name="%s"/>' % bn, sty)
        # store the bookmark name on the flowable so afterFlowable can see this
        h._bookmarkName = bn
        self.story.append(h)

    def parse_labels_csv(self, csv_file):
        csvf = open(csv_file, "rb")
        reader = csv.reader(csvf, delimiter=',')
        extra_labels = []
        for line in reader:
            extra_labels.append((dateutil.parser.parse(line[0]), line[1]))

        return extra_labels

    def graph(self, sar_files, skip_list, output_file='out.pdf', only_categories=[],
              labels=None, show_reboots=False, custom_graphs='', grid_on=False):
        """ Parse sar data and produce graphs of the parsed data. """
        sarobj = self.sar_obj
        self.extra_labels = None
        self.showreboots = show_reboots
        self.grid = grid_on
        doc = MyDocTemplate(output_file, pagesize=landscape(A4))

        if labels is not None:
            try:
                self.extra_labels = []
                for i in labels.strip().split(';'):
                    (time, label) = i.split(',')
                    time = dateutil.parser.parse(time)
                    self.extra_labels.append((time, label))
            except:
                print("Unable to parse extra labels: {0}".format(labels))
                sys.exit(-1)

        self.story.append(Paragraph('%s' % sarobj.hostname, doc.centered))
        self.story.append(Paragraph('%s %s' % (sarobj.kernel, sarobj.version), doc.small_centered))
        self.story.append(Spacer(1, 0.05 * inch))
        self.story.append(Paragraph('%s' % (" ".join(sar_files)), doc.small_centered))
        mins = int(sarobj.sample_frequency / 60)
        secs = int(sarobj.sample_frequency % 60)
        s = "Sampling Frequency: %s minutes" % mins
        if secs > 0:
            s += " %s seconds" % secs
        self.story.append(Paragraph(s, doc.small_centered))

        self.do_heading('Table of contents', doc.centered_index)
        self.story.append(doc.toc)
        self.story.append(PageBreak())

        if len(only_categories) == 0:
            category_order = sar_metadata.list_all_categories()
        else:
            category_order = only_categories

        used_cat = {}
        count = 0
        # Let's create all the images either via multiple threads or in sequence
        if THREADED:
            pool = multiprocessing.Pool(NR_CPUS)
            l = self.graphs_order(category_order, skip_list)
            f = zip(repeat(self), repeat(sarobj), l)
            pool.map(graph_wrapper, f)
        else:
            for dataname in self.graphs_order(category_order, skip_list):
                fname = self._graph_filename(dataname[1][0])
                sarobj.plottimeseries(dataname, fname, self.extra_labels, show_reboots, grid_on)
                sys.stdout.write(".")
                sys.stdout.flush()

        # Custom graphs are always created in non threaded mode as their number is
        # typically quite low. Graph descriptions are in the form:
        # 'foo:ldavg-1,i001/s;bar:i001/s,i002/s'
        custom_graph_list = {}
        if custom_graphs is not '':
            try:
                for i in custom_graphs.split(';'):
                    key = i.split(':')[0]
                    values = i.split(':')[1].split(',')
                    custom_graph_list[key] = values
            except:
                raise Exception("Error in parsing custom graphs: {0}".format(custom_graphs))

            for graph in custom_graph_list.keys():
                graphs = custom_graph_list[graph]
                fname = self._graph_filename(graphs)
                sarobj.plottimeseries((['Custom', None, graphs], graphs), fname, self.extra_labels,
                                      show_reboots, grid_on)
                sys.stdout.write(".")
                sys.stdout.flush()
                fname = self._graph_filename(graphs)
                cat = 'Custom'
                if not cat in used_cat: # We've not seen the category before
                    self.do_heading(cat, doc.h1)
                    used_cat[cat] = True
                else:
                    self.story.append(Paragraph(cat, doc.normal))

                self.do_heading(graph, doc.h2_invisible)
                self.story.append(Image(fname, width=GRAPH_WIDTH*inch, height=GRAPH_HEIGHT*inch))
                self.story.append(Spacer(1, 0.2*inch))

        # All the image files are created let's go through the files and create the pdf
        for dataname in self.graphs_order(category_order, skip_list):
            fname = self._graph_filename(dataname[1][0])
            cat = sarobj._categories[dataname[1][0]]
            title = dataname[0][0]
            # We've not seen the category before
            if not cat in used_cat:
                self.do_heading(cat, doc.h1)
                used_cat[cat] = True
            else:
                self.story.append(Paragraph(cat, doc.normal))
            self.do_heading(title, doc.h2_invisible)
            self.story.append(Image(fname, width=GRAPH_WIDTH*inch, height=GRAPH_HEIGHT*inch))
            self.story.append(Spacer(1, 0.2*inch))
            desc = sar_metadata.get_desc(dataname[1])
            for (name, desc, detail) in desc:
                self.story.append(Paragraph("<strong>%s</strong> - %s" % (name, desc), doc.normal))
                if detail:
                    self.story.append(Paragraph("Counter: <i>%s</i>" % (detail), doc.mono))

            self.story.append(PageBreak())
            count += 1

        doc.multiBuild(self.story)

    def export_csv(self, output_file):
        sarobj = self.sar_obj
        f = open(output_file, 'wb')
        writer = csv.writer(f, delimiter=',')
        all_keys = list(sarobj.available_data_types())
        print(all_keys)
        writer.writerow(all_keys)
        for timestamps in sarobj._data.keys():
            s = []
            for i in all_keys:
                s.append(sarobj._data[timestamps][i])
            writer.writerow(s)

    def graph_ascii(self, graph, def_columns=80, def_rows=25):
        """Displays a single graph in ASCII form on the terminal"""
        import subprocess
        sarobj = self.sar_obj
        timestamps = sorted(sarobj._data.keys())
        dataset = [sarobj._data[d][graph] for d in timestamps]
        try:
            rows, columns = os.popen('stty size', 'r').read().split()
        except:
            columns = def_columns
        rows = def_rows
        if columns > def_columns:
            columns = def_columns
        try:
            gnuplot = subprocess.Popen(["/usr/bin/gnuplot"], stdin=subprocess.PIPE)
        except Exception as e:
            print("Error launching gnuplot: {0}".format(e))
            sys.exit(-1)

        gnuplot.stdin.write("set term dumb {0} {1}\n".format(columns, rows))
        gnuplot.stdin.write("set xdata time\n")
        gnuplot.stdin.write('set xlabel "Time"\n')
        gnuplot.stdin.write('set timefmt \"%Y-%m-%d %H:%M\"\n')
        gnuplot.stdin.write('set xrange [\"%s\":\"%s\"]\n' % (ascii_date(timestamps[0]),
                                                              ascii_date(timestamps[-1])))
        gnuplot.stdin.write('set ylabel "%s"\n' % (graph))
        gnuplot.stdin.write('set datafile separator ","\n')
        gnuplot.stdin.write('set autoscale y\n')
        gnuplot.stdin.write('set title "%s - %s"\n' % (graph, " ".join(sarobj._files)))

        gnuplot.stdin.write("plot '-' using 1:2 title '{0}' with linespoints \n".format(graph))
        for i, j in zip(timestamps, dataset):
            s = '\"%s\",%f\n' % (ascii_date(i), j)
            gnuplot.stdin.write(s)

        gnuplot.stdin.write("e\n")
        gnuplot.stdin.flush()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="{0} - analyzes sar output files and "
                                     "produces a pdf report".format(sys.argv[0]),
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('sar_files', metavar='sar_files', nargs='+', help="""
                        Sar files to examine. It is possible to specify a
                        single folder, in which case it will look for all the
                        sar* files and order them by file date. If the
                        directory containing the sar files is part of an
                        sosreport, it will try to resulve the interrupts
                        source""")

    parser.add_argument('--ascii', dest='ascii_graph', default=None, help="""
                        Display a single graph on the terminal. (Requires
                        installed gnuplot)""")

    parser.add_argument('--category', dest='categories', default=None, help="""
                        Only outputs the selected categories. Available
                        categories: {0}""".format(", ".join(sar_metadata.list_all_categories())))

    parser.add_argument('--csv', dest='csv_file', default=None, help="""
                        Outputs the data to the specified csv file.""")

    parser.add_argument('--custom', dest='custom_graphs', default='', help="""
                        Add custom graphs with --custom
                        'foo:udpsck,rawsck,tcp-tw;bar:i001/s,i002/s'.  This
                        adds two pages in the 'Custome' category: Graph 'foo'
                        containing udpsck, rawsck and tcp-tw and graph 'bar'
                        containing i001/s and i002/s.  With graphs that vary
                        too muchin y-ranges the output can be quite
                        suboptimal""")

    parser.add_argument('--labels', dest='labels', default=None, help="""
                        Adds labels to a graph at specified time. For example
                        --label '2014-01-01 13:45:03,foo; 2014-01-02
                        13:15:15,bar' will add two extra labels on every graph
                        at those times.  This is useful for correlation
                        work""")

    parser.add_argument('--list', dest='list_graphs', action='store_true', help="""
                        Only lists all the graphs and elements contained in the
                        specified sar files""")

    parser.add_argument('--maxgraphs', dest='maxgraphs', default=MAXGRAPHS_IN_PAGE, help="""
                        Sets the maximum number of graphs in a single page""")

    parser.add_argument('--nogrid', dest='grid', action='store_false', default=True, help="""
                        Disable the grid in graphs""")

    parser.add_argument('--output', dest='output_file', default='out.pdf', help="""
                        Output file name""")

    parser.add_argument('--showreboots', dest='showreboots', default=False, action='store_false', help="""
                        If the sar file is part of an existing sosreport, it
                        will try to show the system reboot times in the
                        graphs""")

    parser.add_argument('--skip', default='lo', dest='skip_graphs', help="""
                        Graphs or entries to skip. For example adding '%%user'
                        will remove that graph. Adding 'lo' will remove that
                        interface from any network graph. Use comma to separate
                        multiple graphs to be skipped.  For example '--skip
                        lo,eth0' will not plot any graphs for those interfaces
                        and '--skip lo,miss/s' will skip the 'lo' interface and
                        the 'miss/s' graph""")

    args = parser.parse_args()

    # If the only argument is a directory fetch all the sar files and order
    # them automatically
    if len(args.sar_files) == 1 and os.path.isdir(args.sar_files[0]):
        s = {}
        root = args.sar_files[0]
        files = [f for f in os.listdir(root) if f.startswith('sar')]
        for i in files:
            if i.strip().startswith('sar'):
                f = os.path.join(root, i)
                s[os.path.getmtime(f)] = f

        args.sar_files = []
        for i in sorted(s.keys()):
            args.sar_files.append(s[i])

        if len(args.sar_files) == 0:
            print("No sar files found in dir: {0}".format(root))
            sys.exit(-1)

    print("Parsing files: {0}".format(" ".join(args.sar_files)), end='')
    sar = SAR.SAR(args.sar_files)
    sar.parse()
    print()

    if args.list_graphs:
        print("List of graphs available:")
        print("{0}".format(sorted(sar.available_data_types())))
        timestamps = sorted(sar._data.keys())
        print("Start: {0} - End: {1}".format(timestamps[0], timestamps[-1]))
        sys.exit(0)

    sar_stats = SarStats(sar, int(args.maxgraphs))
    if args.csv_file is not None:
        print("Export to csv: {0}".format(args.csv_file))
        sar_stats.export_csv(args.csv_file)
        sys.exit(0)

    if args.ascii_graph is not None:
        sar_stats.graph_ascii(args.ascii_graph)
        sys.exit(0)

    print("Building graphs: ", end='')

    skip_list = args.skip_graphs.strip().split(",")
    if args.categories is not None:
        selected_categories = args.categories.strip().split(",")
    else:
        selected_categories = []

    sar_stats.graph(args.sar_files, skip_list, args.output_file, selected_categories,
        args.labels, args.showreboots, args.custom_graphs, args.grid)
    sar.close()
    print("\nWrote: {0}".format(args.output_file))
    sar_stats.close()

# vim: autoindent tabstop=4 expandtab smarttab shiftwidth=4 softtabstop=4 tw=0
